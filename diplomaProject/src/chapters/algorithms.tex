\chapter{Recommendation Algorithms}
\label{chapter:recommendation-system-algorithms}
There are 3 types of recommendation algorithms: 
\begin{enumerate}
	\item Recommendations based on content
		 
		These type of recommendations are solely based on the content of the item to be recommended.
		
		It uses various text based algorithms in order to calculate a similarity between two items.

	\item Recommendations based on collaborative filtering 
		
		These type of recommendations are based on previous user history and ratings.
		 
		The main purpose is to find users with simillar interests to a certain user and recommend items that that are preferred by most of the users.
	
	\item Hybrid recommendations

		These type of recommendations combine the previous presented.

\end{enumerate}
All 3 types of algorithms can be used, independently or by combining the obtained results.

In the following sections we are going to present the implemmented algorithms.


\section{Algorithms for Related Articles}
\label{sec:algorithms-for-related-articles}
The related articles recommendations return a list of articles related to a certain article.

We have implemmented both a hybrid approach and recommendations based on content.

A user may choose not to use the hybrid approach and use only the data of the articles.

\subsection{Collaborative Filtering}
\label{sec:collaborative-filtering}
In order to obtain better results, we may choose to use collaborative filtering, in order to do a kind of article grouping by user preferences. Usually, users have a certain profile/preferences, thus they will read articles related to each other. Using this technique we can reduce the number of articles on which we are going to apply the article similarity algorithm. We use the following iterative algorithm\cite{item-to-item-recomm} to find articles related to article A1:
\lstset{language=make,caption=Item to item collaborative filtering,label=lst:app-make}
\begin{lstlisting}
	For each user U that read A1
		For each article A2 read by user U
			Save in common article list that a user read both A1 and A2
	For each article A2 in common article list
		Compute the similarity between A1 and A2
		Save the computed similarity in related article list
	Sort the related article list by similarity in descending order
	Return the related article list
\end{lstlisting}

\subsection{Computing Article Similarity}
\label{sec:computing-article-similarity}

In order to obtain the best related article list we have to take advantage of all the attributes of the articles at our disposal.
We compute the similarity using the:
\begin{itemize}
	\item Date created: used to chose between two articles that have the same related score with the current article
	\item Title: find the similarities in the titles of two articles by using natural language processing
	\item Short title: find the similarities in the short titles of two articles by using natural language processing
	\item Department: check if the articles belong to the same department by comparing the strings
	\item Category: check if the articles belong to the same category by comparing the strings
	\item Importance: articles with the same importance are shown upper in the related articles
	\item Publication: if the articles belong to the same publication the similarity is going to be greater
	\item Language: the articles should be written in the same language in order to be related
	\item Author: if the articles have the same author the similarity is going to be greater
	\item Keywords: if the articles have more common keywords determined by natural language processing analysis then the similarity is going to be greater
	\item Ratings: if the articles have more readers and a higher mean rating, the similarity is going to be greater
	\item Collection Reference: if the articles belong to the same collections the similarity is going to be greater
	\item TFIDF (Term Frequency Inverse Document Frequency) similarity
\end{itemize}
Each of these fields has a certain importance in the final resulted similarity between articles.

\subsubsection{Natural Language Processing}
\label{sec:natural-language-processing}
In order to compute and improve the similarity between two strings we employ the following three methods:
\begin{enumerate}
	\item Levenshtein Distance

		We compute the number of characters needed to change one string to another and divide this value by the length of the bigger string. We obtain a value between 0 and 1.
		
		This method is faster than the next one and is better suited for generating related articles on the fly.

	\item By comparing character pairs
	
		We compute the number of common character pairs of the two strings, multiply it by two and divide it by the number of combined character pairs. In the end, we will obtain a value between 0 and 1.
		
		This method gives better results than the previous one.
	\item Lemmatization

		In order to obtain better TFIDF results we use the Stanford NLP (Natural Language Processing) framework to get the base form of a word.
		
		TODO Write more about it
\end{enumerate}

\subsubsection{Term Frequency Inverse Document Frequency (TFIDF)}
\label{sec:TFIDF}

In order to compute the TFIDF similarity we use the following formulas:
\begin{enumerate}
	\item Computing the term frequency of a term T in a document, D 	
	\begin{equation}
	tf(T, D) = \sqrt[2]{\dfrac{tfreq(T, D)}{tnum(D)}}.
	\end{equation}

		Where tfreq (T, D) is the number of occurrences of a term T in the document D and tnum (D) is the total number of terms in D. A tf value depends on the number of term occurrences. This method cannot characterize documents accurately because it is not able to distinguish important words from trivial words sufficiently. In order to obtain a better TFIDF value we also use stemming and lemmatization on the available words. 
	\item Computing the inverse document frequency of a term T in a document.
	\begin{equation}
	idf(t, D) = \log{\dfrac{N}{\vert t \in D \vert}}.
	\end{equation}

		Where N is the total number of documents in the corpus and $\vert{t \in D }\vert$ is the number of documents where the term t appears. If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the denominator to $1 + \vert{t \in D}\vert$

	\item Computing the TFIDF \cite{hybrid-recomm}
	\begin{equation}
		tfidf (T, D_{1}) = tf (T, D_{1}) \times idf(T,D_{1})
	\end{equation}
	\item Computing the TFIDF similarity

		We use the Cosine similarity in order to do this.
		\begin{equation}
			\begin{split}
				Cosine Similarity(D_{1},D_{2}) = \cos (\theta) = \dfrac{D_{1} \times D_{2}}{\vert\vert D_{1} \vert\vert \times  \vert\vert D_{2} \vert\vert}\\
				= \dfrac {\sum_{i=1}^{n} tfidf(T_{i}, D_{1}) \times tfidf(T_{i}, D_{2})} {\sqrt[2]{\sum_{i=1}^{n} tfidf(T_{i},D_{1} )^{2}} \times \sqrt[2]{\sum_{i=1}^{n} tfidf(T_{i},D_{2} )^{2}}} 
			\end{split}
		\end{equation}
		
		Because the Cosine similarity doesn’t take into account the number of common words or the article size, it does not give good results on it’s own, so, we use the following formula, where D1 is the document for which we want the related article list:
		\begin{equation}
			\begin{split}
				similarity(D_{1},D_{2}) = (Cosine Similarity(D_{1}, D_{2})) \times \\
				\dfrac {(Number Of Common Words(D_{1}, D_{2}))} {max(Number Of Words In D_{1}, Number Of Words In D_{2})} \times \\
				\dfrac {1} {\sqrt[5] {Number Of Words In D_{2}}}
			\end{split}
		\end{equation}

\end{enumerate}


\section{Algorithms for Recommended Articles}
\label{sec:algorithms-for-recommended-articles}

An algorithm used to find users with similar interests and recommend items.

In order to find users with similar interests, we can use the Pearson correlation:
\begin{equation}
	P_{a,u} = \dfrac {\sum_{i=1}^{m}(r_{a,i} - \overline{r_{a}})\times(r_{u,i} - \overline{r_{u}})}{\sqrt[2]{\sum_{i=1}^{m}(r_{a,i} - \overline{r_{a}})^{2}\times \sum_{i=1}^{m}(r_{u,i} - \overline{r_{u}})^{2}}}
\end{equation}
Where r $_{a,i}$ is the rating given by user a to item i and $\overline{r_{a}}$ is the mean rating given by user a to the corated items.

In order to find items that may interest the user, we can use the following formula:
 \begin{equation}
 	p_{a,i} = \overline {r_{a}} + \dfrac{\sum_{u=1}^{n} (r_{u,i} - \overline{r_{u}}) \times P_{a,u}}{\sum_{u=1}^{n} P_{a,u}} 
 \end{equation}
Where $p_{a,i}$ is the predicted rating of user a for item i.
